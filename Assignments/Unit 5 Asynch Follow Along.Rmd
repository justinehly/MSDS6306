---
title: "Unit 5 Asynch"
author: "Justin Ehly"
date: "9/17/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Merging/ Joinging Datasets

```{r}

df1 <- data.frame(c(1234, 2345, 8910, 9101, 3456, 5678, 8888),
                  c("Time_Series", "NLP", "Stats1", "DDS", "DDS", "ML2", "Data Mining"))
names(df1) <- c("Student_ID", "Course")

df2 <- data.frame(c(1234, 2345, 8910, 9101, 3456, 5678,9999, 1111),
                  c("M", "F", "M", "F","F","F","M", "M"),
                  c("TX", "TX", "CA", "ID", "NY", "FL", "NM", "AZ"))
names(df2) <- c("Student_ID", "Gender", "State")

#merge(df1, df2, by = "Student_ID")
#OR
#inner_join(df1, df2, by = "Student_ID")
#Or
#df1%>%inner_join(df2,by="Student_ID")

merge(df1, df2, "Student_ID")

left_join(df1, df2,"Student_ID",all.x = T)

```

#Heat Maps from AcuSpike

```{r}

library(ggplot2)
library(maps)
library(dplyr)
library(mapproj)

Acu= read.csv(file.choose(), header=TRUE) #read in company data
lookup = data.frame(abb = state.abb, State = state.name) #makes a data frame with State name and abbrev
colnames(Acu)[2] = "abb" #change column name
Acu2 = merge(Acu,lookup,"abb") #make one data set with the state names and abb
AcuMapData = count(Acu2, State) # count the occurance of each state
#AcuMapData = AcuMapData[-c(5,9,43),] #shows contrast between other states better
colnames(AcuMapData)[2] = "AcuSpikes" #change "n" to "AcuSpikes"
AcuMapData$region <- tolower(AcuMapData$State)
AcuMapData2 = AcuMapData[-1]
states <- map_data("state")
map.df <- merge(states, AcuMapData2, by = "region", all.x=T)
map.df <- map.df[order(map.df$order),]
ggplot(map.df, aes(x=long, y=lat, group=group))+
  geom_polygon(aes(fill=AcuSpikes))+
  geom_path()+
  scale_fill_gradientn(colours=rev(heat.colors(10)),na.value="grey90")+
  ggtitle("AcuSpikes Systems by State")+
  coord_map()




```


#WDI API and WHO Package
#ISO2C country codes - there is a package for this

```{r}
install.packages("countrycode")
library(countrycode)
countrycode("Albania", 'country.name', 'iso2c')
countrycode("Germany", 'country.name', 'iso2c')
countrycode("United States", 'country.name', 'iso2c')
countrycode("Canada", 'country.name', 'iso2c')

library(WDI)
GDPPC = WDI(indicator="NY.GDP.PCAP.KD", start=2015, end=2015)
head(GDPPC)

library(WHO)
str(codes)
dim(codes)
names(codes)
grep("life expect", codes$display)
codes$display[58]
codes$display[2187]
codes$display[58]
codes$display[58,]
LE = get_data("WHOSIS_000002")
LE$iso2c = countrycode(LE$country, 'country name', 'iso2c')
LE
LE2 = LE %>% filter(sex == "Both sexes" & year == 2015)
LE2
GDPPC = GDPPC[48:264,] #These are the rows of individual countries rather than regions
GDPPC3 = GDPPC2[!is.na(GDPPC2$NY.GDP.PCAP.KD),]
head(GDPPC3)

```

#handling and manipulating strings

```{r}

string1 = "Don't let what you can't do interfere with what you can do."
string2 = "\"Don't let what you can't do interfere with what you can do. \" - John Wooden"
writeLines(string1)
write.csv(string1, "string1.csv")
string3 = c("IF","YOU","GET","GIVE,","IF", "YOU", "LEARN","TEACH", "-MAYA","ANGELOU")
string3

#stringr functions
str_length(string3)

#Collapse Strings
str_c(string3)
#collapse strings... put them together
str_c(string3, collapse = "")
#collapse strings... put them together
str_c(string3, collapse = ",")
#collapse strings...put them together
str_c(string3, collapse = " ")

string4 = str_c(string3, collapse = " ")
string4
str_sub(string4,1,1)
str_sub(string4,1,7)
str_sub(string4,-3,-1)
str_sub(string4,-14,-1)
str_sub(string4,3,7)

```

#Basic Matching

```{r}

string5 = "\"I've missed more than 9000 shots in my career. I've lost almost 300 games. 26 times, I've been trusted to take the game winning shot and missed. I've failed over and over and over again in my life. And that is why I succeed.\" - Michael Jordan"

str_view(string5, "shot")
str_view_all(string5, "shot")
str_view_all(string5, "shot.") # "." is a wildcard character
str_view_all(string5, "missed\\.") # "\\" escape the wildcard and look for "shot" at the end of the sentence
str_view_all(string5, "\\bshot\\b") #\b is the beginning or end of the word starts with an "s" and ends with a "t"
str_view_all(string5, "I")
str_view_all(string5, "\\. I")
str_view_all(string5, "(\\. I| \"I)") #| is or () means a group

```

#Anchors

```{R}

string6 = "\"Sometimes when you innovate, you make mistakes. It is best to admit them quickly, and get with imporving your other innovations.\" - Steve Jobs"
string6
str_view(string1, "^Don't")
str_view(string1, "^Interfere") #matches that start at the front of the string
str_view(string3, "^T") #remember this is a bunch of little strings
str_view(string6, "^\"Some") 
str_view(string6, "Jobs$") #anchor at the end of the string


#find digits and whitespace
str_view(string5,"\\d") #\d says to look for a digit
str_view_all(string5,"\\d")
str_view(string6, "\\s") #\s looks for spaces
str_view_all(string6, "\\s") #\s looks for spaces
str_view(string6,"^ \"Some")
```

#character classes and alternatives

```{r}
#say we have a list of quotes and we want to find those from Steve Jobs
#but we don't know if it is under Steve, Jobs or Steve Jobs
#so we look for Steve or Jobs

df = data.frame(quotes = c(string2, string4, string5, string6), stringsAsFactors = FALSE)
df$quotes
str_view(df$quotes,"(Steve|Jobs)")
str_view_all(df$quotes, "[abc]") #matches abc
str_view_all(df$quotes, "[^abc]") #matches not abc

```

#Repition

```{r}

#?: 0 or 1
#+: 1 or more
#*: 0 or more

str_view(df$quotes, "0+") #greedy
str_view(df$quotes, "[sn]+") #greedy
str_view_all(df$quotes, "[sn]+")

#more repitions
#{n}: exactly n
#{n,}: n or more
#{,m}: at most m

str_view_all(df$quotes, "[sn]{2}" ) #greedy

```

#Detecting matches and counting matches

```{r}

str_view(string5,"shot") #finds "shot"

str_detect(string5, "shot") #returns true or false
str_detect(sting5, "rainbow")

str_count(string5,"shot")
str_count(string5, "rainbow")


#from earlier

str_view_all(string5, "(\\. I| \"I)") #look for period-space-I OR just and I
str_detect(string5, "(\\. I| \"I)") #look for period-space-I OR just and I
str_count(string5, "(\\. I| \"I)") #look for period-space-I OR just and I count the occurances

grepl("and",string5) #similar to str_detect

str_view_all(df$quotes, "shot")
str_detect(df$quotes, "shot")
str_count(df$quotes, "shot")
grepl("shot", df$quotes)


```

#Searching and filtering

```{r}
#searching for authors
df = rbind(df,data.frame(quotes = "\"Success is peace of mind which is a direct result of self-satisfaction in knowing you did your best to become the best you are capable of becoming.\" - John Wooden"))
df = rbind(df, data.frame(quotes = "\"In a gentle way, you can shake the world.\" - Mahatma Gandhi"))

#count quotes by John Wooden
sum(str_detect(df$quotes,"John Wooden"))
#OR
sum(grepl("John Wooden",df$quotes))

#return quotes by john wooden
writeLines(df$quotes[str_detect(df$quotes,"John Wooden")])
#or
writeLines(df$quotes[grepl("John Wooden", df$quotes)])
#or
df %>% filter(str_detect(quotes,"John Wooden"))



```

#extracting and group matching

```{r}

library(stringr)
#Harvard sentences are built into the stringr library
#more information avaialable in wikipedia: https://en.wikipedia.org/wiki/Harvard_sentences

head(sentences, n=10)

colors = c("orange", "blue","yellow", "green", "purple", "brown", "red")
color_expression = str_c(colors, collapse = " | ") #puts  a | in between each color name
color_expression
has_color = str_subset(sentences, color_expression)
has_color
matches = str_extract(has_color, color_expression)
matches
matches_all = str_extract_all(has_color, color_expression, simplify = TRUE)
matches_all
class(matches_all)
matches_all = unlist(str_extract_all(has_color, color_expression))
matches_all = trimws(matches_all)
matchDF = data.frame(Colors = matches_all)
matchDF %>% ggplot(aes(x = Colors, fill = Colors)) + geom_bar()
matchDF %>% ggplot(aes(x = Colors, fill = Colors)) + geom_bar() + 
  scale_fill_manual(values=colors)
matchDF$Colors[order(matchDF$Colors)]
matchDF %>% ggplot(aes(x = Colors, fill = Colors)) + geom_bar() + 
  scale_fill_manual(values=colors[order(colors)])

```


#grouped matches

```{r}

author = "( -| - )([^ ]{2,}) ([^ ]{2,})"
authors = df$quotes

authors %>% str_extract(author)
authors %>% str_match(author)

dfAuthors = data.frame(authors %>% str_match(author))
names(dfAuthors) = c("Full", "Dash", "First", "Last")
dfAuthors
dfAuthors %>% select(c(First, Last))
```

#Replacement
```{r}
roster = data.frame(Name = c("John", "Nancy", "Fred", "Sam", "Julie"),
                    Gender = c("male", "F", "M", "Female", "female"),
                    Major = c("?","Math","Comp Sci","?",""))
str_replace(roster$Gender,"(male|M)", "Male")
str_replace(roster$Gender,"(\\bmale\\b|\\bM\\b)", "Male")
roster$Gender = str_replace(roster$Gender,"(\\male\\b|\\bM\\b)", "Male")
roster$Gender = str_replace(roster$Gender, "(\\bfemale\\b|\\bF\\b|FeMale|feMale)", "Female")
roster$Major = str_replace(roster$Major, "(\\?)", "")


roster

roster$Gender = as.factor(roster$Gender)
roster$Major = as.factor(roster$Major)
levels(roster$Major) = droplevels(roster$Major,"")
roster


str_replace()

```

#Splitting str_split()
```{r}
str_split("Mahatma_Gahndi, John_Wooden, Maya_Angelou", "(_|,)")

unlist(str_split("Mahatma_Gahndi, John_Wooden, Maya_Angelou", "(_|, )"))

s1=sentences[1]
str_split(s1," ")
str_split(s1,"\\b")  #\\b boundary
str_split(s1,boundary("word")) #boundary is a function that knows we just want the words
unlist(str_split(s1,boundary("word")))

```

#regular expressions regex()
```{r}
unlist(str_split("Mahatma_Gahndi, John_Wooden, Maya_Angelou", "(_|, )"))
#is the same as
unlist(str_split("Mahatma_Gahndi, John_Wooden, Maya_Angelou", regex("(_|, )")))

roster = data.frame(Name = c("John", "Nancy", "Fred", "Sam", "Julie", "Pat", "Mel","Jay"),
                    Gender = c("male", "F", "M", "Female", "female", "m","f","Male"),
                    Major = c("NA","Math","Comp Sci","NA","NA","NA","English","EE"))
roster
str(roster)

roster$Name = as.factor(roster$Name)
roster$Gender = as.factor(roster$Gender)
roster$Major = as.factor(roster$Major)
str(roster)

roster$Gender = str_replace(roster$Gender,"(\\bmale\\b|\\bM\\b)", "Male")
roster$Gender = str_replace(roster$Gender,"(\\bfemale\\b|\\bF\\b)", "Female")
roster

roster$Gender = str_replace(roster$Gender, regex("(\\bmale\\b|\\bM\\b)", ignore_case = TRUE), "Male")
roster$Gender = str_replace(roster$Gender, regex("(\\bfemale\\b|\\bF\\b)", ignore_case = TRUE), "Female")
roster

roster[roster$Major=="NA","Major"] <- NA #since this is a factor, R will replace the "NA" with the factor <NA>
roster
```

#NYT Classifier

```{r}
library(dplyr)
library(jsonlite)
library(plyr)
library(tidyr)

NYTIMES_KEY = "s6YwO4tk8ZzronXxbN98MV3Q8aMMgBGa"

#set some parameters
term <- "Taylor Swift" # Need to use + to string together separate words
begin_date<- "20190501"
end_date <- "20190602"

baseurl <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term,"&begin_date=",begin_date,"&end_date=",end_date,          "&facet_filter=true&api-key=",NYTIMES_KEY, sep="")

baseurl

initialQuery <- jsonlite::fromJSON(baseurl)
maxPages <- round((initialQuery$response$meta$hits[1] / 10)-1)

maxPages

pages <- list()
for(i in 0:maxPages){
  nytSearch <- jsonlite::fromJSON(paste0(baseurl, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages[[i+1]] <- nytSearch 
  Sys.sleep(7) # to avoid http 429 error (too many requests)
}           

allNYTSearch <- rbind_pages(pages)


#Segmentation

# Visualize coverage by section
allNYTSearch %>% 
  group_by(response.docs.type_of_material) %>%
  dplyr::summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = "identity") + coord_flip()

#Make another column of News versus Other ... The labels

allNYTSearch$NewsOrOther = ifelse(allNYTSearch$response.docs.type_of_material == "News","News","Other")
#There is an NA in NewsOrOther

# Visualize coverage of News or Other
allNYTSearch[!is.na(allNYTSearch$NewsOrOther),] %>% 
  group_by(NewsOrOther) %>%
  dplyr::summarize(count=n()) %>%
  mutate(percent = (count / sum(count))*100) %>%
  ggplot() +
  geom_bar(aes(y=percent, x=NewsOrOther, fill=NewsOrOther), stat = "identity") + coord_flip()

allNYTSearch$


#Identify Article and Tokenize

ArticleToClassify = allNYTSearch[5,] 
ArticleToClassify$response.docs.headline.main

trueType = ArticleToClassify$NewsOrOther[1]
trueType

library(tm) #text mining library provides the stopwords() function
stopwords()

#The [^[:alnum:] ] replaces all non alphanumeric characters with nulls.  
theText = unlist(str_split(str_replace_all(ArticleToClassify$response.docs.headline.main,"[^[:alnum:] ]", ""), boundary("word"))) #Take out all but alpha numeric characters from search string

theText

wordsToTakeOut = c(stopwords(), "Taylor", "Swift")

#put word boundaries stopwords so that we don't detect partial words later
wordsToTakeOut = str_c(wordsToTakeOut,collapse = "\\b|\\b") 
wordsToTakeOut = str_c("\\b",wordsToTakeOut,"\\b")
wordsToTakeOut

importantWords = theText[!str_detect(theText,regex(wordsToTakeOut,ignore_case = TRUE))]

importantWords

#Find Percentages in News and Other

newsArticles = allNYTSearch %>% filter(NewsOrOther == "News")
otherArticles = allNYTSearch %>% filter(NewsOrOther == "Other")

numNewsArticles = dim(newsArticles)[1]
numOtherArticles = dim(otherArticles)[1]

numNewsArticles
numOtherArticles

thePercentHolderNews = c()
thePercentHolderOther = c()

for(i in 1 : length(importantWords)) #for each important word in the headline
{
  #number of News articles that have the ith word in the headline of interest
  numNews = sum(str_count(newsArticles$response.docs.headline.main[-5],importantWords[i]))
  #number of Other articles that have the ith word in the headline of interest
   numOther = sum(str_count(otherArticles$response.docs.headline.main[-5],importantWords[i]))
 
  #percentage of News articles that have the ith word in the headline of interest 
  thePercentHolderNews[i] = numNews / numNewsArticles
  #percentage of Other articles that have the ith word in the headline of interest
  thePercentHolderOther[i] = numOther / numOtherArticles
  
  #all the News percentages (for each word)
  thePercentHolderNews
  #all the Other percentages (for each word)
  thePercentHolderOther
  
}

thePercentHolderNews
thePercentHolderOther

classifiedAs = if_else(sum(thePercentHolderNews)>sum(thePercentHolderOther),"News","Other")
sum(thePercentHolderNews)
sum(thePercentHolderOther)

Result = str_c("The ", trueType," article was classified as ", classifiedAs, " with a News score of: ",round(sum(thePercentHolderNews),4), " and an Other score of: ", round(sum(thePercentHolderOther),4), ".") 
Result


## VISUALIZE

articleStats = data.frame(Word = importantWords, newsScore = thePercentHolderNews, otherScore = thePercentHolderOther)

# Wide Form / Not Tidy
articleStats

#Tidy and Plot
articleStats[,c(2,3)] %>% gather(Type,Percent) %>% mutate(Word = rep(articleStats$Word,2)) %>% ggplot(aes(y = Percent, x = Type, fill = Word)) + geom_col()

articleStats[,c(2,3)] %>% gather(Type,Percent) %>% mutate(Word = rep(articleStats$Word,2)) %>% ggplot(aes(y = Percent, x = Type, fill = Word)) + geom_col() + facet_wrap(~Word)

articleStats[,c(2,3)] %>% gather(Type,Percent) %>% mutate(Word = rep(articleStats$Word,2)) %>% ggplot(aes(y = Percent, x = Type, fill = Word)) + geom_col() + facet_grid(~Word)

```